---
title: "table.add_embedding_index"
description: "table.add_embedding_index() - Creates a vector similarity index for AI-powered semantic search on text and images"
---

<Badge text="AI/ML" color="orange" size="small" />

## Function Signature

```python
table.add_embedding_index(
    column: Union[str, Column],
    idx_name: Optional[str] = None,
    embedding: Optional[Callable] = None,
    string_embed: Optional[Callable] = None,
    image_embed: Optional[Callable] = None,
    metric: str = 'cosine',
    if_exists: str = 'error'
) -> None
```

## Description

Creates a vector similarity index that enables AI-powered semantic search on text and image columns. Once created, the index automatically stays up-to-date as new rows are inserted, and provides lightning-fast similarity lookups using the `similarity()` pseudo-function.

Embedding indexes unlock powerful AI capabilities like finding visually similar images, semantically related documents, and cross-modal search (finding images using text descriptions). Pixeltable supports both single-modal and multimodal embeddings from popular models like CLIP, Sentence Transformers, and custom UDFs.

## Parameters

<ParamField path="column" type="Union[str, Column]" required>
  The column to create the embedding index on. Must be a `String` or `Image` column. Can be specified by name or column reference.
</ParamField>

<ParamField path="idx_name" type="str" default="None">
  Optional custom name for the index. If not specified, Pixeltable generates a name like `'idx0'`. Must be unique for the table and a valid column name.
</ParamField>

<ParamField path="embedding" type="Callable" default="None">
  The UDF to use for embedding generation. Must accept a single argument (String or Image) and return a fixed-size 1D float array.
</ParamField>

<ParamField path="string_embed" type="Callable" default="None">
  Optional UDF for string embedding component in multimodal setups. Used with `image_embed` to manually construct multimodal embeddings.
</ParamField>

<ParamField path="image_embed" type="Callable" default="None">
  Optional UDF for image embedding component in multimodal setups. Used with `string_embed` to manually construct multimodal embeddings.
</ParamField>

<ParamField path="metric" type="str" default="'cosine'">
  Distance metric for similarity calculations. Options: `'cosine'`, `'ip'` (inner product), or `'l2'` (Euclidean distance).
</ParamField>

<ParamField path="if_exists" type="str" default="'error'">
  How to handle existing indexes with the same name. Options: `'error'`, `'ignore'`, `'replace'`, or `'replace_force'`.
</ParamField>

## Returns

<ResponseField name="None" type="None">
  No return value. The index is created and immediately available for similarity searches.
</ResponseField>

## Examples

### Basic Image Similarity Index

```python
import pixeltable as pxt
from pixeltable.functions.huggingface import clip

# Create table with image data
images = pxt.create_table('product_images', {
    'product_id': pxt.String,
    'image_path': pxt.String,
    'image': pxt.Image
})

# Insert some sample images
images.insert([
    {'product_id': 'PROD_001', 'image_path': 'laptop.jpg', 'image': 'laptop.jpg'},
    {'product_id': 'PROD_002', 'image_path': 'phone.jpg', 'image': 'phone.jpg'},
    {'product_id': 'PROD_003', 'image_path': 'tablet.jpg', 'image': 'tablet.jpg'}
])

# Create CLIP embedding index for semantic image search
embedding_fn = clip.using(model_id='openai/clip-vit-base-patch32')
images.add_embedding_index(images.image, embedding=embedding_fn)

# Now perform similarity search with an image
import PIL.Image
reference_img = PIL.Image.open('query_laptop.jpg')
similarity_score = images.image.similarity(reference_img)

# Find the 5 most similar images
similar_images = images.select(
    images.product_id, 
    images.image_path, 
    similarity_score
).order_by(similarity_score, asc=False).limit(5).collect()

print("Most similar products:")
for row in similar_images:
    print(f"{row['product_id']}: {row['similarity_score']:.3f}")
```

### Cross-Modal Text-to-Image Search

```python
# Using the same CLIP index for text-to-image search
text_query = "a modern laptop computer"
text_similarity = images.image.similarity(text_query)

# Find images matching the text description
matching_images = images.select(
    images.product_id,
    images.image_path,
    text_similarity
).where(text_similarity > 0.7).order_by(text_similarity, asc=False).collect()

print("Images matching 'a modern laptop computer':")
for row in matching_images:
    print(f"{row['product_id']}: {row['text_similarity']:.3f}")
```

### Text Similarity with Sentence Transformers

```python
from pixeltable.functions.huggingface import sentence_transformers

# Create table for document similarity
documents = pxt.create_table('research_papers', {
    'title': pxt.String,
    'abstract': pxt.String,
    'authors': pxt.String,
    'category': pxt.String
})

# Insert research papers
documents.insert([
    {
        'title': 'Attention Is All You Need',
        'abstract': 'A novel neural network architecture based solely on attention mechanisms...',
        'authors': 'Vaswani et al.',
        'category': 'NLP'
    },
    {
        'title': 'BERT: Pre-training of Deep Bidirectional Transformers',
        'abstract': 'We introduce BERT, a new method for pre-training language models...',
        'authors': 'Devlin et al.',
        'category': 'NLP'
    }
])

# Create semantic text index
text_embedding = sentence_transformers.using(
    model_id='all-MiniLM-L6-v2'
)
documents.add_embedding_index(
    documents.abstract, 
    embedding=text_embedding,
    idx_name='abstract_similarity'
)

# Find papers similar to a query
query = "transformer architecture for language understanding"
semantic_sim = documents.abstract.similarity(query)

related_papers = documents.select(
    documents.title,
    documents.authors,
    semantic_sim
).where(semantic_sim > 0.6).order_by(semantic_sim, asc=False).collect()
```

### Multiple Indexes with Different Metrics

```python
# Create multiple indexes on the same column with different metrics
images.add_embedding_index(
    images.image,
    idx_name='cosine_similarity',
    embedding=embedding_fn,
    metric='cosine'
)

images.add_embedding_index(
    images.image,
    idx_name='inner_product',
    embedding=embedding_fn,
    metric='ip'
)

images.add_embedding_index(
    images.image,
    idx_name='euclidean_distance',
    embedding=embedding_fn,
    metric='l2'
)

# Use different metrics for different use cases
cosine_sim = images.image.similarity(query_img)  # Uses default cosine
```

### Multimodal Index with Separate Embeddings

```python
from pixeltable.functions.openai import text_embedding
from pixeltable.functions.huggingface import clip

# Create separate embedding functions
text_embed_fn = text_embedding.using(model='text-embedding-ada-002')
image_embed_fn = clip.using(model_id='openai/clip-vit-base-patch32')

# Create multimodal index manually
documents.add_embedding_index(
    documents.description,  # Text column
    idx_name='multimodal_search',
    string_embed=text_embed_fn,
    image_embed=image_embed_fn
)
```

### Production AI Search System

```python
import pixeltable as pxt
from pixeltable.functions.huggingface import clip, sentence_transformers

# E-commerce product catalog with AI search
catalog = pxt.create_table('product_catalog', {
    'sku': pxt.String,
    'name': pxt.String,
    'description': pxt.String,
    'category': pxt.String,
    'price': pxt.Float,
    'main_image': pxt.Image,
    'thumbnail': pxt.Image
})

try:
    # Multi-index strategy for different search patterns
    
    # Visual similarity for image-based search
    visual_embedding = clip.using(model_id='openai/clip-vit-large-patch14')
    catalog.add_embedding_index(
        catalog.main_image,
        idx_name='visual_similarity',
        embedding=visual_embedding,
        metric='cosine'
    )
    
    # Text similarity for description search
    text_embedding = sentence_transformers.using(
        model_id='all-mpnet-base-v2'
    )
    catalog.add_embedding_index(
        catalog.description,
        idx_name='text_similarity', 
        embedding=text_embedding,
        metric='cosine'
    )
    
    # Product name similarity for fuzzy matching
    name_embedding = sentence_transformers.using(
        model_id='all-MiniLM-L6-v2'
    )
    catalog.add_embedding_index(
        catalog.name,
        idx_name='name_similarity',
        embedding=name_embedding,
        metric='cosine'
    )
    
    print("✅ AI search indexes created successfully!")
    
    # Test cross-modal product search
    query_text = "wireless bluetooth headphones"
    
    # Search by description similarity
    desc_sim = catalog.description.similarity(query_text)
    
    # Search by name similarity  
    name_sim = catalog.name.similarity(query_text)
    
    # Combined relevance scoring
    results = catalog.select(
        catalog.sku,
        catalog.name,
        catalog.price,
        desc_sim.alias('description_score'),
        name_sim.alias('name_score')
    ).where(
        (desc_sim > 0.5) | (name_sim > 0.6)
    ).order_by(desc_sim, asc=False).limit(10).collect()
    
    print(f"🔍 Found {len(results)} relevant products")
    
except Exception as e:
    print(f"❌ Index creation failed: {e}")
```

### Advanced Similarity Search Patterns

```python
# Multi-step similarity refinement
user_query_image = PIL.Image.open('user_upload.jpg')

# Step 1: Broad similarity search
broad_similarity = catalog.main_image.similarity(user_query_image)
candidates = catalog.select(catalog.sku, broad_similarity).where(
    broad_similarity > 0.6
).collect()

# Step 2: Category filtering
electronics = catalog.where(
    (catalog.category == 'electronics') & 
    (broad_similarity > 0.7)
).select(catalog.sku, catalog.name, broad_similarity).collect()

# Step 3: Price-aware similarity ranking
affordable_similar = catalog.where(
    (broad_similarity > 0.65) & 
    (catalog.price < 200.0)
).select(
    catalog.sku, 
    catalog.name, 
    catalog.price,
    broad_similarity
).order_by(broad_similarity, asc=False).limit(5).collect()
```

## Similarity Search Performance

### Index Creation Time

| Data Type | 10K Items | 100K Items | 1M Items |
|-----------|-----------|------------|----------|
| Text (384D) | 2-5 seconds | 20-45 seconds | 3-8 minutes |
| Images (512D) | 3-8 seconds | 30-60 seconds | 5-12 minutes |
| Multimodal | 5-12 seconds | 45-90 seconds | 8-20 minutes |

### Query Performance

| Query Type | Latency | Throughput |
|------------|---------|------------|
| Single similarity | 1-5ms | 200-1000 QPS |
| Top-K search | 5-20ms | 50-200 QPS |
| Filtered similarity | 10-50ms | 20-100 QPS |

## Best Practices

### **Choose the Right Embedding Model:**
- **CLIP**: Cross-modal text/image search
- **Sentence Transformers**: Pure text similarity
- **Custom UDFs**: Domain-specific embeddings
- **OpenAI Embeddings**: High-quality general purpose

### **Optimize Index Performance:**
- Use appropriate similarity metrics for your use case
- Consider multiple indexes for different search patterns
- Monitor index size vs. query performance trade-offs
- Batch similarity queries when possible

### **Production Considerations:**
- Index creation is compute-intensive - plan accordingly
- Embeddings consume significant storage (4-8 bytes per dimension)
- Consider embedding dimension trade-offs (accuracy vs. speed)
- Test similarity thresholds with your specific data

## Raises

**Error**: If an index with the specified name already exists and `if_exists='error'`, or if the specified column does not exist.

**TypeError**: If the embedding function doesn't return the expected array format.

**ValueError**: If the metric is not one of the supported options.

## Related Functions

- [`add_index`](./add_index) - Create traditional database indexes for exact matching
- [`drop_embedding_index`](./drop_embedding_index) - Remove vector similarity indexes
- [`find_embedding_index`](./find_embedding_index) - Discover existing embedding indexes

---

*In a world drowning in data, similarity is the lighthouse that guides us to meaning. Every embedding index is a compass pointing toward relevance.*